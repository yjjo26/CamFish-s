# -*- coding: utf-8 -*-
"""
CamFish AI Data Collector
=========================
ì›¹ ê²€ìƒ‰ì„ í†µí•´ ë‚šì‹œ/ìº í•‘ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  Geminië¡œ ë¶„ì„í•˜ì—¬ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python data_collector.py

í™˜ê²½ ë³€ìˆ˜ (ë˜ëŠ” .env íŒŒì¼):
    - GEMINI_API_KEY: Google Gemini API í‚¤
    - SUPABASE_URL: Supabase í”„ë¡œì íŠ¸ URL
    - SUPABASE_KEY: Supabase anon/service key
"""

import os
import re
import json
import time
import uuid
from typing import List, Dict, Any, Optional

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# Gemini & Supabase ë¼ì´ë¸ŒëŸ¬ë¦¬
import google.generativeai as genai
from supabase import create_client, Client

# DuckDuckGo ê²€ìƒ‰ (ë¬´ë£Œ)
from duckduckgo_search import DDGS


# ==============================================================================
# í™˜ê²½ ì„¤ì •
# ==============================================================================

load_dotenv()

GEMINI_API_KEY = os.getenv("VITE_GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY")
SUPABASE_URL = os.getenv("VITE_SUPABASE_URL") or os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("VITE_SUPABASE_KEY") or os.getenv("SUPABASE_KEY")

if not GEMINI_API_KEY:
    raise EnvironmentError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not SUPABASE_URL or not SUPABASE_KEY:
    raise EnvironmentError("SUPABASE_URL ë˜ëŠ” SUPABASE_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# Gemini ì„¤ì •
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash')

# Supabase í´ë¼ì´ì–¸íŠ¸
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# HTTP í—¤ë” (í¬ë¡¤ë§ ì°¨ë‹¨ ë°©ì§€)
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}


# ==============================================================================
# ê²€ìƒ‰ì–´ ëª©ë¡
# ==============================================================================

SEARCH_QUERIES = [
    # ë‚šì‹œ í¬ì¸íŠ¸
    "ì†¡ì • í•´ìˆ˜ìš•ì¥ ë‚šì‹œ í¬ì¸íŠ¸",
    "íƒœì•ˆ ëª½ì‚°í¬ í•´ë³€ ë‚šì‹œ",
    "ì‹œí™”ë°©ì¡°ì œ ë‚šì‹œ í¬ì¸íŠ¸",
    "ì„ì™•ë¦¬ ì„ ë…€ë°”ìœ„ ìš°ëŸ­ ë‚šì‹œ",
    "ê¶í‰í•­ í”¼ì‹±í”¼ì–´ ë§ë‘¥ì–´",
    "ê°•ë¦‰ ì£¼ë¬¸ì§„ ë°©íŒŒì œ ë‚šì‹œ",
    "ë¶€ì‚° ê¸°ì¥ ê°¯ë°”ìœ„ ë‚šì‹œ",
    "ì—¬ìˆ˜ ëŒì‚°ë„ ê°ì„±ë”",
    "ì œì£¼ ì„œê·€í¬ ë‚šì‹œ í¬ì¸íŠ¸",
    "ëŒ€ì²œí•´ìˆ˜ìš•ì¥ ë‚šì‹œ",
    # ìº í•‘ì¥
    "ê°€í‰ ìë¼ì„¬ ìº í•‘ì¥",
    "íƒœì•ˆ ëª½ì‚°í¬ ìº í•‘",
    "ê°•ë¦‰ ê²½í¬ëŒ€ ì˜¤í† ìº í•‘",
    "ì†ì´ˆ ì˜ë‘í˜¸ ìº í•‘",
    "ì–‘ì–‘ ì„œí”¼ë¹„ì¹˜ ìº í•‘",
]


# ==============================================================================
# ì›¹ ê²€ìƒ‰ (DuckDuckGo)
# ==============================================================================

def search_web(query: str, max_results: int = 5) -> List[str]:
    """
    DuckDuckGoë¡œ ê²€ìƒ‰í•˜ì—¬ ìƒìœ„ URLë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, region='kr-kr', max_results=max_results))
            urls = [r['href'] for r in results if 'href' in r]
            print(f"  ğŸ” '{query}' â†’ {len(urls)}ê°œ URL ë°œê²¬")
            return urls
    except Exception as e:
        print(f"  âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []


# ==============================================================================
# ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
# ==============================================================================

def scrape_page(url: str, timeout: int = 10) -> Optional[str]:
    """
    URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    try:
        response = requests.get(url, headers=HEADERS, timeout=timeout)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ìŠ¤í¬ë¦½íŠ¸, ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            tag.decompose()
        
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (article ë˜ëŠ” main íƒœê·¸ ìš°ì„ )
        main_content = soup.find('article') or soup.find('main') or soup.find('div', class_=re.compile(r'content|post|entry'))
        if main_content:
            text = main_content.get_text(separator='\n', strip=True)
        else:
            text = soup.get_text(separator='\n', strip=True)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬ (ë„ˆë¬´ ê¸´ ê²½ìš° ìë¥´ê¸°)
        text = re.sub(r'\n{3,}', '\n\n', text)  # ê³¼ë„í•œ ì¤„ë°”ê¿ˆ ì œê±°
        text = text[:8000]  # Gemini ì»¨í…ìŠ¤íŠ¸ ì œí•œ ê³ ë ¤
        
        return text if len(text) > 100 else None
        
    except Exception as e:
        print(f"    âš ï¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ({url[:50]}...): {e}")
        return None


# ==============================================================================
# Gemini AI ë¶„ì„
# ==============================================================================

EXTRACTION_PROMPT = """
ë‹¹ì‹ ì€ í•œêµ­ì˜ ë‚šì‹œ/ìº í•‘ ì •ë³´ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ í…ìŠ¤íŠ¸ì—ì„œ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ **JSON í˜•ì‹**ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.

ì¶”ì¶œí•  ì •ë³´:
1. place_name: ì¥ì†Œëª… (ì˜ˆ: "ì„ì™•ë¦¬ ì„ ë…€ë°”ìœ„", "ëª½ì‚°í¬ í•´ìˆ˜ìš•ì¥")
2. place_type: "FISHING" ë˜ëŠ” "CAMPING" (ë‚šì‹œ ê´€ë ¨ì´ë©´ FISHING, ìº í•‘ ê´€ë ¨ì´ë©´ CAMPING)
3. address: ì£¼ì†Œ (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
4. lat: ì¶”ì • ìœ„ë„ (ìˆ«ì, ì—†ìœ¼ë©´ null)
5. lng: ì¶”ì • ê²½ë„ (ìˆ«ì, ì—†ìœ¼ë©´ null)
6. description: ì¥ì†Œ ì„¤ëª… (50ì ì´ë‚´)
7. fish_species: ì£¼ìš” ì–´ì¢… ë°°ì—´ (ë‚šì‹œì¸ ê²½ìš°ë§Œ, ì˜ˆ: ["ìš°ëŸ­", "ê´‘ì–´", "ë…¸ë˜ë¯¸"])
8. recommended_baits: ì¶”ì²œ ë¯¸ë¼ ë°°ì—´ (ë‚šì‹œì¸ ê²½ìš°ë§Œ, ì˜ˆ: ["ê°¯ì§€ë ì´", "í¬ë¦´ìƒˆìš°"])
9. nearby_places: ì£¼ë³€ í¸ì˜ì‹œì„¤/ë‚šì‹œì  ë°°ì—´ [{name, type, address}] (type: "BAIT_SHOP", "RESTAURANT", "CONVENIENCE_STORE")

ì‘ë‹µì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ í•´ë‹¹ í•„ë“œë¥¼ ë¹ˆ ê°’ ë˜ëŠ” nullë¡œ ì„¤ì •í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
---
{text}
---

JSON ì‘ë‹µ:
"""

def analyze_with_gemini(text: str, query: str) -> Optional[Dict[str, Any]]:
    """
    Geminië¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ë‚šì‹œ/ìº í•‘ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    try:
        prompt = EXTRACTION_PROMPT.format(text=text)
        response = model.generate_content(prompt)
        
        # JSON íŒŒì‹±
        raw_text = response.text.strip()
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì œê±°
        if raw_text.startswith('```'):
            raw_text = re.sub(r'^```(?:json)?\n?', '', raw_text)
            raw_text = re.sub(r'\n?```$', '', raw_text)
        
        data = json.loads(raw_text)
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not data.get('place_name'):
            # ê²€ìƒ‰ì–´ì—ì„œ ì¥ì†Œëª… ì¶”ì¶œ ì‹œë„
            data['place_name'] = query.split()[0] if query else None
        
        return data
        
    except json.JSONDecodeError as e:
        print(f"    âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None
    except Exception as e:
        print(f"    âš ï¸ Gemini ë¶„ì„ ì‹¤íŒ¨: {e}")
        return None


# ==============================================================================
# Supabase ì €ì¥
# ==============================================================================

def check_place_exists(place_name: str) -> bool:
    """
    DBì— í•´ë‹¹ ì¥ì†Œê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    try:
        result = supabase.table('places').select('id').eq('name', place_name).execute()
        return len(result.data) > 0
    except Exception:
        return False


def save_to_database(data: Dict[str, Any]) -> bool:
    """
    ì¶”ì¶œëœ ë°ì´í„°ë¥¼ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    place_name = data.get('place_name')
    if not place_name:
        print("    âš ï¸ ì¥ì†Œëª…ì´ ì—†ì–´ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False
    
    # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    if check_place_exists(place_name):
        print(f"    â„¹ï¸ '{place_name}'ì€(ëŠ”) ì´ë¯¸ DBì— ì¡´ì¬í•©ë‹ˆë‹¤. ê±´ë„ˆëœ€.")
        return False
    
    try:
        # 1. places í…Œì´ë¸”ì— ì €ì¥
        place_data = {
            'name': place_name,
            'type': data.get('place_type', 'FISHING'),
            'address': data.get('address', ''),
            'description': data.get('description', ''),
        }
        
        # ì¢Œí‘œê°€ ìˆìœ¼ë©´ PostGIS Pointë¡œ ë³€í™˜
        lat = data.get('lat')
        lng = data.get('lng')
        if lat and lng:
            place_data['location'] = f"SRID=4326;POINT({lng} {lat})"
        
        result = supabase.table('places').insert(place_data).execute()
        
        if not result.data:
            print(f"    âš ï¸ places ì €ì¥ ì‹¤íŒ¨")
            return False
        
        place_id = result.data[0]['id']
        print(f"    âœ… '{place_name}' ì €ì¥ ì™„ë£Œ (ID: {place_id[:8]}...)")
        
        # 2. ì–´ì¢… ì •ë³´ ì €ì¥ (fish_species + location_species_map)
        fish_species = data.get('fish_species', [])
        for species_name in fish_species:
            save_fish_species(species_name, place_id)
        
        # 3. ì£¼ë³€ ì‹œì„¤ ì €ì¥
        nearby_places = data.get('nearby_places', [])
        for nearby in nearby_places:
            save_nearby_place(nearby, lat, lng)
        
        return True
        
    except Exception as e:
        print(f"    âš ï¸ DB ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


def save_fish_species(species_name: str, place_id: str):
    """
    ì–´ì¢…ì„ fish_species í…Œì´ë¸”ì— ì €ì¥í•˜ê³  location_species_mapì— ì—°ê²°í•©ë‹ˆë‹¤.
    """
    try:
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        result = supabase.table('fish_species').select('id').eq('korean_name', species_name).execute()
        
        if result.data:
            species_id = result.data[0]['id']
        else:
            # ìƒˆ ì–´ì¢… ì¶”ê°€
            new_species = {
                'korean_name': species_name,
                'scientific_name': '',
                'habitat': 'SALTWATER',  # ê¸°ë³¸ê°’
                'active_months': [1,2,3,4,5,6,7,8,9,10,11,12],
            }
            insert_result = supabase.table('fish_species').insert(new_species).execute()
            if not insert_result.data:
                return
            species_id = insert_result.data[0]['id']
        
        # location_species_mapì— ì—°ê²°
        mapping = {
            'place_id': place_id,
            'species_id': species_id,
            'season_specific': 'ì—°ì¤‘',
        }
        supabase.table('location_species_map').upsert(mapping, on_conflict='place_id,species_id').execute()
        
    except Exception as e:
        print(f"      âš ï¸ ì–´ì¢… ì €ì¥ ì‹¤íŒ¨ ({species_name}): {e}")


def save_nearby_place(nearby: Dict[str, Any], base_lat: float = None, base_lng: float = None):
    """
    ì£¼ë³€ í¸ì˜ì‹œì„¤ì„ places í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    name = nearby.get('name')
    if not name or check_place_exists(name):
        return
    
    try:
        place_data = {
            'name': name,
            'type': 'AMENITY',
            'address': nearby.get('address', ''),
            'description': f"{nearby.get('type', 'í¸ì˜ì‹œì„¤')}",
        }
        
        # ê¸°ì¤€ ì¢Œí‘œì—ì„œ ì•½ê°„ ì˜¤í”„ì…‹ (ì •í™•í•œ ì¢Œí‘œ ì—†ì„ ë•Œ)
        if base_lat and base_lng:
            import random
            offset = random.uniform(-0.01, 0.01)
            place_data['location'] = f"SRID=4326;POINT({base_lng + offset} {base_lat + offset})"
        
        supabase.table('places').insert(place_data).execute()
        
    except Exception as e:
        print(f"      âš ï¸ ì£¼ë³€ì‹œì„¤ ì €ì¥ ì‹¤íŒ¨ ({name}): {e}")


# ==============================================================================
# ë©”ì¸ ì‹¤í–‰
# ==============================================================================

def process_query(query: str) -> int:
    """
    ë‹¨ì¼ ê²€ìƒ‰ì–´ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ë°˜í™˜: ì €ì¥ëœ ì¥ì†Œ ìˆ˜
    """
    print(f"\n{'='*60}")
    print(f"ğŸ” ê²€ìƒ‰ì–´: {query}")
    print('='*60)
    
    # 1. ì›¹ ê²€ìƒ‰
    urls = search_web(query, max_results=5)
    if not urls:
        return 0
    
    # 2. ê° URL ìŠ¤í¬ë˜í•‘ ë° ë¶„ì„
    saved_count = 0
    combined_text = ""
    
    for i, url in enumerate(urls[:3], 1):  # ìƒìœ„ 3ê°œë§Œ ì²˜ë¦¬
        print(f"  [{i}] {url[:60]}...")
        text = scrape_page(url)
        if text:
            combined_text += f"\n\n---ì¶œì²˜ {i}---\n{text}"
            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
    
    if not combined_text:
        print("  âš ï¸ ìœ íš¨í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return 0
    
    # 3. AI ë¶„ì„
    print("  ğŸ¤– Gemini ë¶„ì„ ì¤‘...")
    data = analyze_with_gemini(combined_text, query)
    
    if data:
        # 4. DB ì €ì¥
        if save_to_database(data):
            saved_count += 1
    
    return saved_count


def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("="*60)
    print("ğŸ£ CamFish AI Data Collector ì‹œì‘")
    print("="*60)
    print(f"ğŸ“‹ ê²€ìƒ‰ì–´ ìˆ˜: {len(SEARCH_QUERIES)}ê°œ")
    print(f"ğŸ”— Supabase: {SUPABASE_URL[:40]}...")
    
    total_saved = 0
    
    for query in SEARCH_QUERIES:
        try:
            saved = process_query(query)
            total_saved += saved
            time.sleep(2)  # API ì†ë„ ì œí•œ ë°©ì§€
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
            break
        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    print("\n" + "="*60)
    print(f"âœ… ì™„ë£Œ! ì´ {total_saved}ê°œ ì¥ì†Œê°€ ìƒˆë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*60)


if __name__ == "__main__":
    main()
